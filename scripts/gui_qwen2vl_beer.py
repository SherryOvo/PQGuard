#!/usr/bin/env python3
"""
Qwen2-VL å¤šæ¨¡æ€æ¨¡å‹ GUI ç•Œé¢

åŠŸèƒ½ï¼š
- æ–‡æœ¬å¯¹è¯
- å›¾åƒè¯†åˆ«å’Œé—®ç­”
- è¯­éŸ³è¾“å…¥ï¼ˆéº¦å…‹é£å½•éŸ³ï¼‰
- è¯­éŸ³è¾“å‡ºï¼ˆTTSï¼‰

ä½¿ç”¨ Gradio æ„å»º Web ç•Œé¢
"""

import sys
import os

# åœ¨å¯¼å…¥ Gradio ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…å¯åŠ¨æ£€æŸ¥é—®é¢˜
os.environ["GRADIO_SERVER_NAME"] = "0.0.0.0"
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
os.environ["GRADIO_IS_COLAB_HOST"] = "False"
os.environ["GRADIO_IS_SPACES"] = "False"

from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import tempfile
import io

# è®¾ç½®æ¨¡å‹ç¯å¢ƒå˜é‡ï¼ˆåœ¨å¯¼å…¥ transformers ä¹‹å‰ï¼‰
sys.path.insert(0, str(Path(__file__).parent))
import env_model

import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image
import gradio as gr

# è¯­éŸ³ç›¸å…³
try:
    import whisper
    import soundfile as sf
    import edge_tts
    import asyncio
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
    print("è­¦å‘Šï¼šè¯­éŸ³è¯†åˆ«åŠŸèƒ½æœªå®‰è£…ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

# éº¦å…‹é£å½•éŸ³ï¼ˆå¯é€‰ï¼ŒæœåŠ¡å™¨ç¯å¢ƒå¯èƒ½ä¸éœ€è¦ï¼‰
try:
    import pyaudio
    import wave
    MICROPHONE_AVAILABLE = True
except ImportError:
    MICROPHONE_AVAILABLE = False
    print("æç¤ºï¼špyaudioæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨éº¦å…‹é£å½•éŸ³ï¼Œä½†ä»å¯é€šè¿‡ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ä½¿ç”¨è¯­éŸ³åŠŸèƒ½")


# è‡ªåŠ¨æŸ¥æ‰¾è®­ç»ƒå¥½çš„æ¨¡å‹
_MODEL_DIR = Path("outputs/qwen2vl_beer_sft")
_BASE_MODEL_ID = "Qwen/Qwen2-VL-7B-Instruct"

if _MODEL_DIR.exists() and (_MODEL_DIR / "config.json").exists():
    MODEL_ID = str(_MODEL_DIR)
    print(f"âœ“ ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹: {MODEL_ID}")
else:
    MODEL_ID = _BASE_MODEL_ID
    print(f"âš  ä½¿ç”¨åŸºç¡€æ¨¡å‹: {MODEL_ID}")


# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œå¤„ç†å™¨
processor = None
model = None
chat_history: List[Dict[str, Any]] = []


def load_model():
    """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ã€‚"""
    global processor, model
    
    if processor is not None and model is not None:
        return processor, model
    
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_ID}...")
    
    processor = AutoProcessor.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
    )
    
    model = AutoModelForVision2Seq.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else {"": "cpu"},
    )
    model.eval()
    
    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    return processor, model


def generate_reply(
    text: str,
    image: Optional[Image.Image] = None,
    max_new_tokens: int = 512,
) -> str:
    """ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒå›¾åƒè¾“å…¥ï¼‰ã€‚"""
    global processor, model, chat_history
    
    if processor is None or model is None:
        processor, model = load_model()
    
    # å‡†å¤‡æ¶ˆæ¯
    if not chat_history or chat_history[0].get("role") != "system":
        system_prompt = "ä½ æ˜¯ä¸€åç²¾é€šç²¾é…¿å•¤é…’å·¥è‰ºã€è®¾å¤‡ç®¡ç†å’Œå¼‚å¸¸è¯Šæ–­çš„ä¸­æ–‡æ™ºèƒ½åŠ©æ‰‹ï¼Œæ”¯æŒå›¾åƒè¯†åˆ«å’Œè¯­éŸ³äº¤äº’ã€‚"
        chat_history = [{"role": "system", "content": system_prompt}]
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    user_message = {"role": "user", "content": text}
    chat_history.append(user_message)
    
    # å‡†å¤‡è¾“å…¥
    if image is not None:
        # å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰
        messages_text = processor.apply_chat_template(
            chat_history,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = processor(
            text=messages_text,
            images=[image],
            return_tensors="pt",
        )
    else:
        # çº¯æ–‡æœ¬è¾“å…¥
        messages_text = processor.apply_chat_template(
            chat_history,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = processor(
            text=messages_text,
            return_tensors="pt",
        )
    
    # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
    inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
    
    # ç”Ÿæˆ
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.8,
            top_p=0.8,
        )
    
    # è§£ç 
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs["input_ids"], generated_ids)
    ]
    response = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )[0]
    
    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
    chat_history.append({"role": "assistant", "content": response})
    
    return response


def text_chat(message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:
    """æ–‡æœ¬å¯¹è¯å¤„ç†ã€‚"""
    if not message.strip():
        return "", history
    
    try:
        response = generate_reply(message)
        history.append((message, response))
        return "", history
    except Exception as e:
        error_msg = f"é”™è¯¯: {str(e)}"
        history.append((message, error_msg))
        return "", history


def image_chat(message: str, image: Image.Image, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:
    """å›¾åƒ+æ–‡æœ¬å¯¹è¯å¤„ç†ã€‚"""
    if image is None:
        return "è¯·å…ˆä¸Šä¼ å›¾ç‰‡", history
    
    if not message.strip():
        message = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡å¹¶å›ç­”ç›¸å…³é—®é¢˜ã€‚"
    
    try:
        response = generate_reply(message, image=image)
        history.append((f"[å›¾åƒ] {message}", response))
        return "", history
    except Exception as e:
        error_msg = f"é”™è¯¯: {str(e)}"
        history.append((f"[å›¾åƒ] {message}", error_msg))
        return "", history


# Gradio çš„ Audio ç»„ä»¶è‡ªå¸¦å½•éŸ³åŠŸèƒ½ï¼Œä¸éœ€è¦å•ç‹¬çš„å½•éŸ³å‡½æ•°


def transcribe_audio(audio_path: str) -> str:
    """å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬ã€‚"""
    if not SPEECH_AVAILABLE:
        return "è¯­éŸ³è¯†åˆ«åŠŸèƒ½æœªå®‰è£…"
    
    if audio_path is None:
        return "æœªæä¾›éŸ³é¢‘æ–‡ä»¶"
    
    try:
        whisper_model = whisper.load_model("base")
        result = whisper_model.transcribe(audio_path, language="zh")
        return result["text"]
    except Exception as e:
        return f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}"


def voice_chat(audio_input, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:
    """è¯­éŸ³å¯¹è¯å¤„ç†ã€‚"""
    if audio_input is None:
        return "è¯·å…ˆå½•åˆ¶æˆ–ä¸Šä¼ éŸ³é¢‘", history
    
    # Gradio Audio ç»„ä»¶è¿”å›çš„å¯èƒ½æ˜¯å…ƒç»„ (filepath, sample_rate) æˆ–åªæ˜¯ filepath
    if isinstance(audio_input, tuple):
        audio_path = audio_input[0]
    else:
        audio_path = audio_input
    
    if not audio_path or not os.path.exists(audio_path):
        return "éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨", history
    
    try:
        # è¯­éŸ³è½¬æ–‡æœ¬
        text = transcribe_audio(audio_path)
        if not text or text.startswith("è¯­éŸ³è¯†åˆ«å¤±è´¥") or text.startswith("æœªæä¾›"):
            return text, history
        
        # ç”Ÿæˆå›å¤
        response = generate_reply(text)
        history.append((f"[è¯­éŸ³] {text}", response))
        return "", history
    except Exception as e:
        error_msg = f"é”™è¯¯: {str(e)}"
        history.append(("è¯­éŸ³è¾“å…¥", error_msg))
        return "", history


def clear_history():
    """æ¸…ç©ºå¯¹è¯å†å²ã€‚"""
    global chat_history
    chat_history = []
    return []


def text_to_speech(text: str) -> Optional[str]:
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ã€‚"""
    if not SPEECH_AVAILABLE:
        return None
    
    try:
        import edge_tts
        import asyncio
        
        async def _tts_async():
            communicate = edge_tts.Communicate(text, "zh-CN-XiaoxiaoNeural")
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            await communicate.save(temp_file.name)
            return temp_file.name
        
        audio_path = asyncio.run(_tts_async())
        return audio_path
    except Exception as e:
        print(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
        return None


# åˆ›å»º Gradio ç•Œé¢
def create_interface():
    """åˆ›å»º Gradio ç•Œé¢ã€‚"""
    
    # é¢„åŠ è½½æ¨¡å‹
    print("æ­£åœ¨é¢„åŠ è½½æ¨¡å‹...")
    load_model()
    
    # åˆ›å»º Gradio ç•Œé¢ï¼ˆä¸ä½¿ç”¨ theme å‚æ•°ä»¥å…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
    with gr.Blocks(title="ç²¾é…¿å•¤é…’æ™ºèƒ½åŠ©æ‰‹ - Qwen2-VL") as demo:
        gr.Markdown("""
        # ğŸº ç²¾é…¿å•¤é…’æ™ºèƒ½åŠ©æ‰‹
        
        åŸºäº Qwen2-VL å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç²¾é…¿å•¤é…’çŸ¥è¯†é—®ç­”ç³»ç»Ÿ
        
        **åŠŸèƒ½ï¼š**
        - ğŸ“ æ–‡æœ¬å¯¹è¯ï¼šç›´æ¥è¾“å…¥é—®é¢˜
        - ğŸ–¼ï¸ å›¾åƒè¯†åˆ«ï¼šä¸Šä¼ å›¾ç‰‡è¿›è¡Œé—®ç­”
        - ğŸ¤ è¯­éŸ³è¾“å…¥ï¼šä½¿ç”¨éº¦å…‹é£å½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
        - ğŸ”Š è¯­éŸ³è¾“å‡ºï¼šå°†å›å¤è½¬æ¢ä¸ºè¯­éŸ³
        """)
        
        with gr.Tabs():
            # Tab 1: æ–‡æœ¬å¯¹è¯
            with gr.Tab("ğŸ“ æ–‡æœ¬å¯¹è¯"):
                text_chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=500,
                )
                with gr.Row():
                    text_input = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æµ‘æµŠIPAï¼Ÿå¦‚ä½•åˆ¤æ–­å‘é…µæ˜¯å¦å®Œæˆï¼Ÿ",
                        scale=4,
                    )
                    text_submit = gr.Button("å‘é€", variant="primary", scale=1)
                
                text_clear = gr.Button("æ¸…ç©ºå†å²", variant="secondary")
                
                text_submit.click(
                    text_chat,
                    inputs=[text_input, text_chatbot],
                    outputs=[text_input, text_chatbot],
                )
                text_input.submit(
                    text_chat,
                    inputs=[text_input, text_chatbot],
                    outputs=[text_input, text_chatbot],
                )
                text_clear.click(clear_history, outputs=[text_chatbot])
            
            # Tab 2: å›¾åƒè¯†åˆ«
            with gr.Tab("ğŸ–¼ï¸ å›¾åƒè¯†åˆ«"):
                image_chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=400,
                )
                with gr.Row():
                    with gr.Column(scale=1):
                        image_input = gr.Image(
                            label="ä¸Šä¼ å›¾ç‰‡",
                            type="pil",
                        )
                    with gr.Column(scale=2):
                        image_text_input = gr.Textbox(
                            label="é—®é¢˜ï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šè¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œåˆ¤æ–­é…µæ¯æ´»æ€§æ˜¯å¦æ­£å¸¸ï¼Ÿ",
                            lines=3,
                        )
                        image_submit = gr.Button("å‘é€", variant="primary")
                
                image_clear = gr.Button("æ¸…ç©ºå†å²", variant="secondary")
                
                image_submit.click(
                    image_chat,
                    inputs=[image_text_input, image_input, image_chatbot],
                    outputs=[image_text_input, image_chatbot],
                )
                image_clear.click(clear_history, outputs=[image_chatbot])
            
            # Tab 3: è¯­éŸ³å¯¹è¯
            with gr.Tab("ğŸ¤ è¯­éŸ³å¯¹è¯"):
                voice_chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=400,
                )
                with gr.Row():
                    with gr.Column():
                        if SPEECH_AVAILABLE:
                            gr.Markdown("**æ–¹å¼1ï¼šä½¿ç”¨éº¦å…‹é£å½•éŸ³**")
                            voice_audio_input = gr.Audio(
                                label="å½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶",
                                type="filepath",
                                sources=["microphone", "upload"],
                            )
                        else:
                            gr.Markdown("âš ï¸ è¯­éŸ³åŠŸèƒ½æœªå®‰è£…ï¼Œè¯·å®‰è£…ï¼š`pip install openai-whisper pyaudio soundfile`")
                            voice_audio_input = gr.Audio(
                                label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶",
                                type="filepath",
                                sources=["upload"],
                            )
                        voice_submit = gr.Button("å‘é€", variant="primary")
                
                voice_clear = gr.Button("æ¸…ç©ºå†å²", variant="secondary")
                
                voice_submit.click(
                    voice_chat,
                    inputs=[voice_audio_input, voice_chatbot],
                    outputs=[voice_audio_input, voice_chatbot],
                )
                voice_clear.click(clear_history, outputs=[voice_chatbot])
            
            # Tab 4: è¯­éŸ³è¾“å‡º
            with gr.Tab("ğŸ”Š è¯­éŸ³è¾“å‡º"):
                tts_input = gr.Textbox(
                    label="è¾“å…¥æ–‡æœ¬",
                    placeholder="è¾“å…¥è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬",
                    lines=5,
                )
                tts_output = gr.Audio(label="ç”Ÿæˆçš„è¯­éŸ³")
                tts_submit = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")
                
                def generate_tts(text):
                    if not text.strip():
                        return None
                    audio_path = text_to_speech(text)
                    return audio_path if audio_path else None
                
                tts_submit.click(
                    generate_tts,
                    inputs=[tts_input],
                    outputs=[tts_output],
                )
        
        gr.Markdown("""
        ---
        **ä½¿ç”¨æç¤ºï¼š**
        - æ–‡æœ¬å¯¹è¯ï¼šç›´æ¥è¾“å…¥é—®é¢˜å³å¯
        - å›¾åƒè¯†åˆ«ï¼šä¸Šä¼ å›¾ç‰‡åå¯ä»¥è¾“å…¥é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å‘é€è®©æ¨¡å‹è‡ªåŠ¨åˆ†æ
        - è¯­éŸ³è¾“å…¥ï¼šç‚¹å‡»å½•éŸ³æŒ‰é’®æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«å¹¶å›ç­”
        - è¯­éŸ³è¾“å‡ºï¼šè¾“å…¥æ–‡æœ¬åç‚¹å‡»ç”Ÿæˆè¯­éŸ³ï¼Œå¯ä»¥å¬åˆ°å›å¤
        """)
    
    return demo


def get_server_ip():
    """è·å–æœåŠ¡å™¨IPåœ°å€ã€‚"""
    import socket
    try:
        # è¿æ¥åˆ°ä¸€ä¸ªå¤–éƒ¨åœ°å€æ¥è·å–æœ¬æœºIP
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except Exception:
        return "localhost"


def main():
    """ä¸»å‡½æ•°ã€‚"""
    print("=" * 60)
    print("å¯åŠ¨ Qwen2-VL å¤šæ¨¡æ€æ¨¡å‹ GUI ç•Œé¢")
    print("=" * 60)
    
    demo = create_interface()
    
    # è·å–æœåŠ¡å™¨IP
    server_ip = get_server_ip()
    server_port = 7860
    
    print("\n" + "=" * 60)
    print("æœåŠ¡å™¨é…ç½®ä¿¡æ¯ï¼š")
    print(f"  æœåŠ¡å™¨IP: {server_ip}")
    print(f"  ç«¯å£: {server_port}")
    print(f"  è®¿é—®åœ°å€: http://{server_ip}:{server_port}")
    print("=" * 60)
    print("\næ³¨æ„ï¼š")
    print("  1. å¦‚æœæ˜¯è¿œç¨‹æœåŠ¡å™¨ï¼Œè¯·ä½¿ç”¨ä¸Šè¿°IPåœ°å€è®¿é—®")
    print("  2. ç¡®ä¿é˜²ç«å¢™å·²å¼€æ”¾ç«¯å£ 7860")
    print("  3. å¦‚æœæ— æ³•è®¿é—®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œé…ç½®")
    print("\næ­£åœ¨å¯åŠ¨ç•Œé¢...\n")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¦ç”¨å¯åŠ¨æ£€æŸ¥
    import os
    os.environ["GRADIO_SERVER_NAME"] = "0.0.0.0"
    os.environ["GRADIO_SERVER_PORT"] = str(server_port)
    
    # å¯åŠ¨ç•Œé¢
    print("å¯åŠ¨ Gradio ç•Œé¢...\n")
    print("=" * 60)
    print("é‡è¦æç¤ºï¼š")
    print("1. å¦‚æœå¯åŠ¨å¤±è´¥ï¼Œè¯·ä½¿ç”¨ SSH ç«¯å£è½¬å‘è®¿é—®")
    print(f"   åœ¨æœ¬åœ°è¿è¡Œ: ssh -L 7860:localhost:7860 root@{server_ip}")
    print("   ç„¶åè®¿é—®: http://localhost:7860")
    print("2. æˆ–è€…ä½¿ç”¨å…¬å…±é“¾æ¥ï¼ˆshare=Trueï¼‰")
    print("=" * 60)
    print()
    
    # å°è¯•ä½¿ç”¨ share=True å¯åŠ¨ï¼ˆåˆ›å»ºå…¬å…±é“¾æ¥ï¼‰
    try:
        demo.launch(
            share=True,  # åˆ›å»ºå…¬å…±é“¾æ¥ï¼Œç»•è¿‡æœ¬åœ°æ£€æŸ¥
            server_port=server_port,
            show_error=True,
            inbrowser=False,
        )
    except Exception as e:
        print(f"\nä½¿ç”¨ share=True å¯åŠ¨å¤±è´¥: {e}")
        print("\nå°è¯•ä½¿ç”¨æœ¬åœ°æœåŠ¡å™¨æ¨¡å¼ï¼ˆéœ€è¦ SSH ç«¯å£è½¬å‘ï¼‰...")
        print(f"SSH å‘½ä»¤: ssh -L {server_port}:localhost:{server_port} root@{server_ip}\n")
        # å›é€€åˆ°æœ¬åœ°æ¨¡å¼
        demo.launch(
            server_name="127.0.0.1",
            server_port=server_port,
            share=False,
            show_error=True,
            inbrowser=False,
        )


if __name__ == "__main__":
    main()

